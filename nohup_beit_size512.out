WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Loading pretrained weights from Hugging Face hub (timm/beit_large_patch16_512.in22k_ft_in22k_in1k)
Loading pretrained weights from Hugging Face hub (timm/beit_large_patch16_512.in22k_ft_in22k_in1k)
Model beit_large_patch16_512_in22k_ft_in22k_in1k created, param count:305674728
Data processing configuration for current model + dataset:
	input_size: (3, 512, 512)
	interpolation: bicubic
	mean: (0.5, 0.5, 0.5)
	std: (0.5, 0.5, 0.5)
	crop_pct: 1.0
	crop_mode: center
Using native Torch AMP. Training in mixed precision.
Using native Torch DistributedDataParallel.
Scheduled epochs: 100. LR stepped per epoch.
Train: 0 [   0/5000 (  0%)]  Loss: 7.434 (7.43)  Time: 3.158s,    2.53/s  (3.158s,    2.53/s)  LR: 1.000e-05  Data: 0.376 (0.376)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/5000 (  1%)]  Loss: 8.461 (7.95)  Time: 0.375s,   21.31/s  (0.427s,   18.75/s)  LR: 1.000e-05  Data: 0.003 (0.010)
Train: 0 [ 100/5000 (  2%)]  Loss: 8.100 (8.00)  Time: 0.376s,   21.30/s  (0.401s,   19.94/s)  LR: 1.000e-05  Data: 0.003 (0.007)
Train: 0 [ 150/5000 (  3%)]  Loss: 8.440 (8.11)  Time: 0.378s,   21.14/s  (0.393s,   20.33/s)  LR: 1.000e-05  Data: 0.003 (0.006)
Train: 0 [ 200/5000 (  4%)]  Loss: 7.910 (8.07)  Time: 0.390s,   20.49/s  (0.392s,   20.42/s)  LR: 1.000e-05  Data: 0.003 (0.005)
Train: 0 [ 250/5000 (  5%)]  Loss: 8.301 (8.11)  Time: 0.383s,   20.91/s  (0.391s,   20.47/s)  LR: 1.000e-05  Data: 0.003 (0.005)
Train: 0 [ 300/5000 (  6%)]  Loss: 8.488 (8.16)  Time: 0.382s,   20.92/s  (0.389s,   20.55/s)  LR: 1.000e-05  Data: 0.003 (0.004)
Train: 0 [ 350/5000 (  7%)]  Loss: 7.269 (8.05)  Time: 0.381s,   20.98/s  (0.388s,   20.60/s)  LR: 1.000e-05  Data: 0.003 (0.004)
Train: 0 [ 400/5000 (  8%)]  Loss: 7.737 (8.02)  Time: 0.387s,   20.68/s  (0.387s,   20.65/s)  LR: 1.000e-05  Data: 0.003 (0.004)
Train: 0 [ 450/5000 (  9%)]  Loss: 7.576 (7.97)  Time: 0.381s,   21.02/s  (0.387s,   20.69/s)  LR: 1.000e-05  Data: 0.003 (0.004)
Train: 0 [ 500/5000 ( 10%)]  Loss: 7.680 (7.95)  Time: 0.379s,   21.10/s  (0.386s,   20.73/s)  LR: 1.000e-05  Data: 0.003 (0.004)
Train: 0 [ 550/5000 ( 11%)]  Loss: 7.737 (7.93)  Time: 0.379s,   21.11/s  (0.385s,   20.76/s)  LR: 1.000e-05  Data: 0.003 (0.004)
Train: 0 [ 600/5000 ( 12%)]  Loss: 7.558 (7.90)  Time: 0.379s,   21.13/s  (0.385s,   20.79/s)  LR: 1.000e-05  Data: 0.003 (0.004)
WARNING:torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2018435 closing signal SIGHUP
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2018436 closing signal SIGHUP
Traceback (most recent call last):
  File "/home/changhao/.conda/envs/pytorch1131_p38/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 237, in launch_agent
    result = agent.run()
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/changhao/.conda/envs/pytorch1131_p38/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2018317 got signal: 1
